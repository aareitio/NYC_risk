<!DOCTYPE html>
<html>
<head>
  <title>Measuring Risk in NYC</title>
  <meta charset="utf-8">
  <meta name="description" content="Measuring Risk in NYC">
  <meta name="author" content="Andy Areitio, Claudia Kampbel, Pedro B Franco (INSEAD MBA Class of July 2017); Jorge Bravo">
  <meta name="generator" content="slidify" />
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <link rel="stylesheet" href="http://slidifylibraries2.googlecode.com/git/inst/libraries/frameworks/io2012/css/default.css" media="all" >
  <link rel="stylesheet" href="http://slidifylibraries2.googlecode.com/git/inst/libraries/frameworks/io2012/css/phone.css" 
    media="only screen and (max-device-width: 480px)" >
  <link rel="stylesheet" href="http://slidifylibraries2.googlecode.com/git/inst/libraries/frameworks/io2012/css/slidify.css" >
  <link rel="stylesheet" href="http://slidifylibraries2.googlecode.com/git/inst/libraries/highlighters/highlight.js/css/tomorrow.css" />
  <base target="_blank"> <!-- This amazingness opens all links in a new tab. -->  
  
  <!-- Grab CDN jQuery, fall back to local if offline -->
  <script src="http://ajax.aspnetcdn.com/ajax/jQuery/jquery-1.7.min.js"></script>
  <script>window.jQuery || document.write('<script src="http://slidifylibraries2.googlecode.com/git/inst/libraries/widgets/quiz/js/jquery.js"><\/script>')</script> 
  <script data-main="http://slidifylibraries2.googlecode.com/git/inst/libraries/frameworks/io2012/js/slides" 
    src="http://slidifylibraries2.googlecode.com/git/inst/libraries/frameworks/io2012/js/require-1.0.8.min.js">
  </script>
  
  

</head>
<body style="opacity: 0">
  <slides class="layout-widescreen">
    
    <!-- LOGO SLIDE -->
        <slide class="title-slide segue nobackground">
  <hgroup class="auto-fadein">
    <h1>Measuring Risk in NYC</h1>
    <h2></h2>
    <p>Andy Areitio, Claudia Kampbel, Pedro B Franco (INSEAD MBA Class of July 2017); Jorge Bravo<br/></p>
  </hgroup>
  <article></article>  
</slide>
    

    <!-- SLIDES -->
    <slide class="" id="slide-1" style="background:;">
  <hgroup>
    <h1>Project Objective:</h1>
  </hgroup>
  <article data-timings="">
    <p>We intend to analyze and identify patterns regarding car accidents in New York City (&quot;NYC&quot;). In order to acheive this objective we have obtained from the New York Police Department (&quot;NYPD&quot;) open source data with almost two years of accidents. The table has 29 columns containing details on each accident reported to the NYPD in these two years.</p>

<p>Discalimer: Please note that in order to run this project on a local computer, you will have do download the files in the GITHUB repository and extract the CSV file from the NYPD_Motor_Vehicle_Collisions_2.csv.zip</p>

<p>Please name the file: data/NYPD_Motor_Vehicle_Collisions_Official.CSV</p>

<p>Thank you!</p>

<pre><code class="r"># basic preparatory commands

# clear the working environment
rm(list = ls())

# load the dataset for the project
NYPDfile &lt;- read.csv(&quot;./data/NYPD_Motor_Vehicle_Collisions_Official.CSV&quot;, sep=&quot;,&quot;, dec=&quot;.&quot;)

# load required packages
# install required packages if loading failed
# all packages sould be listed inside install_load()
source(&quot;./library.R&quot;)
#---install_load(&quot;ggplot2&quot;, &quot;corrplot&quot;, &quot;caret&quot;, &quot;C50&quot;, &quot;cwhmisc&quot;, &quot;stringr&quot;, &quot;gmodels&quot;)
</code></pre>

<h2>Project process and document structure</h2>

<p>Please find below the process we have used to solve the proposed business problem:</p>

<p><strong>1. Defining business problem.</strong> As described above, our intentions are to analyze and identify patterns concerning car accidents in the city of New York. We visualize our final output, for this project, as a tool that will indicate, on a simple scale, the danger of a certain postal code given a certain day of the week and time.</p>

<p><strong>2. Data undestanding.</strong> We have downloaded open source data from the following site:
(<a href="https://data.cityofnewyork.us/Public-Safety/NYPD-Motor-Vehicle-Collisions/h9gi-nx95">https://data.cityofnewyork.us/Public-Safety/NYPD-Motor-Vehicle-Collisions/h9gi-nx95</a>).</p>

<p>Each line of the table represents an incident involving a motor car vehicle in NYC from [xxx] to [xxx].</p>

<p>Please find below a breif description of the columns:
1. DATE (Floating Time Stamp) - Date of the incident
2. TIME (Text) - Time the incident occured
3. BOROUGH (Text) - The Bourough the accident occured
4. ZIP.CODE (Text) - The Zipcode of the location
5. LATITUDE (Number #) - Latitude of the location the accident occured
6. LONGITUDE (Number #) - Longitude of the location the accident occured
7. LOCATION (Location Datatype) - Location, where logitude and latitude are joined and can be refferenced to a map
8. ON.STREET.NAME (Plain text) - Name of the street the accident occured<br>
9. CROSS.STREET.NAME (Plain text) - Closest cross street<br>
10. OFF.STREET.NAME (Plain text)<br>
11. NUMBER.OF.PERSONS.INJURED (Number #) - Total number of injured in accident<br>
12. NUMBER.OF.PERSONS.KILLED (Number #) - Total number of fatalities<br>
13. NUMBER.OF.PEDESTRIANS.INJURED (Number #) - Pedestrians injured
14. NUMBER.OF.PEDESTRIANS.KILLED (Number #) - Pedestrian fatalities<br>
15. NUMBER.OF.CYCLIST.INJURED (Number #) - Cyclists injured<br>
16. NUMBER.OF.CYCLIST.KILLED (Number #) - Cyclists with fatalities<br>
17. NUMBER.OF.MOTORIST.INJURED (Number #) - Motorists injured<br>
18. NUMBER.OF.MOTORIST.KILLED (Number #) - MOtorists killed<br>
19. CONTRIBUTING.FACTOR.VEHICLE.1 (Plain text) 
20. CONTRIBUTING.FACTOR.VEHICLE.2 (Plain text)
21. CONTRIBUTING.FACTOR.VEHICLE.3 (Plain text)
22. CONTRIBUTING.FACTOR.VEHICLE.4 (Plain text)
23. CONTRIBUTING.FACTOR.VEHICLE.5 (Plain text)
24. UNIQUE.KEY  (Number #) - Unique key for the table<br>
25. VEHICLE.TYPE.CODE.1 (Plain text) - Type of Vehicle 1 (sports utility, taxi, passenger, bus, etc)<br>
26. VEHICLE.TYPE.CODE.2 (Plain text)<br>
27. VEHICLE.TYPE.CODE.3 (Plain text)<br>
28. VEHICLE.TYPE.CODE.4 (Plain text)<br>
29. VEHICLE.TYPE.CODE.5 (Plain text)         </p>

<p><strong>3. Data preparation.</strong> Preparation of the dataset for subsequent modeling.</p>

<ul>
<li>Check for missing values, exclusion of corresponding observations.</li>
<li>Check for outliers, decision on their participation in analysis.</li>
<li>Conversion of non-numerical attributes to numerical dummy variables.</li>
<li>Range normalization (if required).</li>
<li>Dimensionality reduction (if required).</li>
<li>Separation of dataset into training and test.</li>
</ul>

<p><strong>4. Modelling.</strong> Building the predictive model based on training dataset with target attribute answering the chosen business question. </p>

<p>For classification trees methodology, modelling process include following steps.</p>

<ul>
<li>Choice of methodology and tool.</li>
<li>First application of tool to dataset.</li>
<li>Generation of decision tree visualization, decision on simplification possibility.</li>
<li>Simplification of the model if possible.</li>
<li>Preliminary evaluation of the model using confusion matrix and test dataset. Decision to proceed (if evaluation results are satisfactory) or repeating the modelling process.</li>
</ul>

<p><strong>5. Evaluation.</strong> Check of models predictive accuracy on test dataset.</p>

<ul>
<li>Generation of confusion matrix using built model and test dataset.</li>
<li>Decision on accepting the model or repeating the modelling process.</li>
</ul>

<p><strong>6. Deployment.</strong> Conclusion on project results and description of potential deployment of built model in practice. Remarks about potential ways to improve process and model of this project.</p>

<h1>Business Understanding</h1>

<p>People are the most important resource for a lot of companies. And employees turnover has always been one of major HR issues. Retaining workers is becoming a balancing act between hard data research and a human touch. Itâ€™s more important than ever to complement intuition with statistical analysis (more on this available, for example on <a href="http://knowledge.insead.edu/blog/insead-blog/the-art-of-keeping-employees-from-leaving-3896">INSEAD Knowledge</a>.</p>

<p>Business problem that this project intends to solve is <strong>prediction of employees to leave in near future using attributes known about each of employees</strong>.</p>

<h1>Data Understanding</h1>

<h2>Source of data</h2>

<p>For this project we use dataset &#39;Human Resources Analytics&#39; published by Ludovic Benistant on kaggle (<a href="https://www.kaggle.com/ludobenistant/hr-analytics/">source</a>) under CC BY-SA 4.0 License. </p>

<h2>Dataset size and variables</h2>

<ul>
<li>Employee satisfaction level <em>(satisfaction_level)</em></li>
<li>Last evaluation <em>(last_evaluation)</em></li>
<li>Number of projects <em>(number_project)</em></li>
<li>Average monthly hours <em>(average_montly_hours)</em></li>
<li>Time spent at the company <em>(time_spend_company)</em></li>
<li>Whether they have had a work accident <em>(Work_accident)</em></li>
<li>Whether they have had a promotion in the last 5 years <em>(promotion_last_5years)</em></li>
<li>Department <em>(sales)</em></li>
<li>Salary <em>(salary)</em></li>
<li>Whether the employee has left, the target variable in this project <em>(left)</em></li>
</ul>

<p>Below is summary of project dataset structure.</p>

<pre><code class="r">colnames(NYPDfile)
</code></pre>

<pre><code>##  [1] &quot;DATE&quot;                          &quot;TIME&quot;                         
##  [3] &quot;BOROUGH&quot;                       &quot;ZIP.CODE&quot;                     
##  [5] &quot;LATITUDE&quot;                      &quot;LONGITUDE&quot;                    
##  [7] &quot;LOCATION&quot;                      &quot;ON.STREET.NAME&quot;               
##  [9] &quot;CROSS.STREET.NAME&quot;             &quot;OFF.STREET.NAME&quot;              
## [11] &quot;NUMBER.OF.PERSONS.INJURED&quot;     &quot;NUMBER.OF.PERSONS.KILLED&quot;     
## [13] &quot;NUMBER.OF.PEDESTRIANS.INJURED&quot; &quot;NUMBER.OF.PEDESTRIANS.KILLED&quot; 
## [15] &quot;NUMBER.OF.CYCLIST.INJURED&quot;     &quot;NUMBER.OF.CYCLIST.KILLED&quot;     
## [17] &quot;NUMBER.OF.MOTORIST.INJURED&quot;    &quot;NUMBER.OF.MOTORIST.KILLED&quot;    
## [19] &quot;CONTRIBUTING.FACTOR.VEHICLE.1&quot; &quot;CONTRIBUTING.FACTOR.VEHICLE.2&quot;
## [21] &quot;CONTRIBUTING.FACTOR.VEHICLE.3&quot; &quot;CONTRIBUTING.FACTOR.VEHICLE.4&quot;
## [23] &quot;CONTRIBUTING.FACTOR.VEHICLE.5&quot; &quot;UNIQUE.KEY&quot;                   
## [25] &quot;VEHICLE.TYPE.CODE.1&quot;           &quot;VEHICLE.TYPE.CODE.2&quot;          
## [27] &quot;VEHICLE.TYPE.CODE.3&quot;           &quot;VEHICLE.TYPE.CODE.4&quot;          
## [29] &quot;VEHICLE.TYPE.CODE.5&quot;
</code></pre>

<h2>Distributions of variables {.tabset}</h2>

<p>First, let&#39;s generate summary report for all variables.</p>

<pre><code class="r">#---summary(HR)
</code></pre>

<p>We will visualize distributions of variables separately for people who left the company and who stayed to be able to see differences between these groups of people on charts before doing the actual modelling.</p>

<pre><code class="r">#---HR.left &lt;- subset(HR, left == 1)
#---HR.stayed &lt;- subset(HR, left == 0)
</code></pre>

<p>There are [ ] observations of employees who left the company and [ ] observations of employees who stayed.</p>

<h3>Satisfaction level</h3>

<pre><code class="r">#---ggplot(HR.left, aes(HR.left$satisfaction_level)) + geom_density(kernel = &quot;gaussian&quot;, fill = &#39;#B8274C&#39;, alpha = 0.3) + labs(x = &quot;Satisfaction level of employees who left&quot;) + xlim(0, 1)
</code></pre>

<pre><code class="r">#---ggplot(HR.stayed, aes(HR.stayed$satisfaction_level)) + geom_density(kernel = &quot;gaussian&quot;, fill = &#39;#006E51&#39;, alpha = 0.3) + labs(x = &quot;Satisfaction level of employees who stayed&quot;) + xlim(0, 1)
</code></pre>

<h3>Last evaluation</h3>

<pre><code class="r">#---ggplot(HR.left, aes(HR.left$last_evaluation)) + geom_density(kernel = &quot;gaussian&quot;, fill = &#39;#B8274C&#39;, alpha = 0.3) + labs(x = &quot;Last evaluation of employees who left&quot;) + xlim(0, 1)
</code></pre>

<pre><code class="r">#---ggplot(HR.stayed, aes(HR.stayed$last_evaluation)) + geom_density(kernel = &quot;gaussian&quot;, fill = &#39;#006E51&#39;, alpha = 0.3) + labs(x = &quot;Last evaluation of employees who stayed&quot;) + xlim(0, 1)
</code></pre>

<h3>Number of projects</h3>

<pre><code class="r">#---ggplot(HR.left, aes(HR.left$number_project)) + geom_histogram(fill = &#39;#B8274C&#39;, binwidth = .5) + scale_x_continuous(breaks = seq(0,max(max(HR.left$number_project),max(HR.stayed$number_project)),by = 1), limits = c(0, max(max(HR.left$number_project),max(HR.stayed$number_project))+1)) + labs(x = &quot;Number of projects of employees who left&quot;)
</code></pre>

<pre><code class="r">#---ggplot(HR.stayed, aes(HR.stayed$number_project)) + geom_histogram(fill = &#39;#006E51&#39;, binwidth = .5) + scale_x_continuous(breaks = seq(0,max(max(HR.left$number_project),max(HR.stayed$number_project)),by = 1), limits = c(0, max(max(HR.left$number_project),max(HR.stayed$number_project))+1)) + labs(x = &quot;Number of projects of employees who stayed&quot;)
</code></pre>

<h3>Monthly hours</h3>

<pre><code class="r">#---ggplot(HR.left, aes(HR.left$average_montly_hours)) + geom_density(kernel = &quot;gaussian&quot;, fill = &#39;#B8274C&#39;, alpha = 0.3) + labs(x = &quot;Average monthly hours of employees who left&quot;) + xlim(min( min(HR.left$average_montly_hours), min(HR.stayed$average_montly_hours)), max( max(HR.left$average_montly_hours), max(HR.stayed$average_montly_hours)))
</code></pre>

<pre><code class="r">#---ggplot(HR.stayed, aes(HR.stayed$average_montly_hours)) + geom_density(kernel = &quot;gaussian&quot;, fill = &#39;#006E51&#39;, alpha = 0.3) + labs(x = &quot;Average monthly hours of employees who stayed&quot;) + xlim(min( min(HR.left$average_montly_hours), min(HR.stayed$average_montly_hours)), max( max(HR.left$average_montly_hours), max(HR.stayed$average_montly_hours)))
</code></pre>

<h3>Time at company</h3>

<pre><code class="r">#---ggplot(HR.left, aes(HR.left$time_spend_company)) + geom_histogram(fill = &#39;#B8274C&#39;, binwidth = .5) + scale_x_continuous(breaks = seq(0,max(max(HR.left$time_spend_company),max(HR.stayed$time_spend_company)),by = 1), limits = c(0, max(max(HR.left$time_spend_company),max(HR.stayed$time_spend_company))+1)) + labs(x = &quot;Time spent at the company (in years) of employees who left&quot;)
</code></pre>

<pre><code class="r">#---ggplot(HR.stayed, aes(HR.stayed$time_spend_company)) + geom_histogram(fill = &#39;#006E51&#39;, binwidth = .5) + scale_x_continuous(breaks = seq(0,max(max(HR.left$time_spend_company),max(HR.stayed$time_spend_company)),by = 1), limits = c(0, max(max(HR.left$time_spend_company),max(HR.stayed$time_spend_company))+1)) + labs(x = &quot;Time spent at the company (in years) of employees who stayed&quot;)
</code></pre>

<h3>Work accidents</h3>

<pre><code class="r">#---ggplot(HR.left, aes(HR.left$Work_accident)) + geom_histogram(fill = &#39;#B8274C&#39;, binwidth = .5) + labs(x = &quot;Number of employees who experienced and did not experience work accident of employees who left&quot;)
</code></pre>

<pre><code class="r">#---ggplot(HR.stayed, aes(HR.stayed$Work_accident)) + geom_histogram(fill = &#39;#006E51&#39;, binwidth = .5) + labs(x = &quot;Number of employees who experienced and did not experience work accident of employees who stayed&quot;)
</code></pre>

<h3>Promotions</h3>

<pre><code class="r">#---ggplot(HR.left, aes(HR.left$promotion_last_5years)) + geom_histogram(fill = &#39;#B8274C&#39;, binwidth = .5) + labs(x = &quot;Number of employees who were or were not promoted during last 5 years of employees who left&quot;)
</code></pre>

<pre><code class="r">#---ggplot(HR.stayed, aes(HR.stayed$promotion_last_5years)) + geom_histogram(fill = &#39;#006E51&#39;, binwidth = .5) + labs(x = &quot;Number of employees who were or were not promoted during last 5 years of employees who stayed&quot;)
</code></pre>

<h3>Department</h3>

<pre><code class="r">#---Departments &lt;- c(&quot;sales&quot;, &quot;technical&quot;, &quot;support&quot;, &quot;IT&quot;, &quot;product_mng&quot;, &quot;marketing&quot;, &quot;RandD&quot;, &quot;accounting&quot;, &quot;hr&quot;,&quot;management&quot;)
#---ggplot(HR.left, aes(HR.left$sales)) + geom_bar(stat = &quot;count&quot;, fill = &#39;#B8274C&#39;) + labs(x = &quot;Department of employees who left&quot;) + scale_x_discrete(limits = Departments)
</code></pre>

<pre><code class="r">#---ggplot(HR.stayed, aes(HR.stayed$sales)) + geom_bar(stat = &quot;count&quot;, fill = &#39;#006E51&#39;) + labs(x = &quot;Department of employees who stayed&quot;) + scale_x_discrete(limits = Departments)
</code></pre>

<h3>Salary</h3>

<pre><code class="r">#---ggplot(HR.left, aes(HR.left$salary)) + geom_bar(stat = &quot;count&quot;, fill = &#39;#B8274C&#39;) + labs(x = &quot;Salary level of employees who left&quot;) + scale_x_discrete(limits = c(&quot;low&quot;, &quot;medium&quot;, &quot;high&quot;))
</code></pre>

<pre><code class="r">#---ggplot(HR.stayed, aes(HR.stayed$salary)) + geom_bar(stat = &quot;count&quot;, fill = &#39;#006E51&#39;) + labs(x = &quot;Salary level of employees who stayed&quot;) + scale_x_discrete(limits = c(&quot;low&quot;, &quot;medium&quot;, &quot;high&quot;))
</code></pre>

<h2>Correlations</h2>

<p>Let&#39;s check correlations between variables.</p>

<pre><code class="r">#---CorTable &lt;- cor(HR[,1:8])
#---corrplot(CorTable, method = &quot;number&quot;, type= &quot;upper&quot;)
</code></pre>

<h1>Data preparation</h1>

<h2>Check for missing values</h2>

<p>Let&#39;s count missing values in the dataset.</p>

<pre><code class="r">#---sum(is.na(HR))
</code></pre>

<p>Since we don&#39;t have missing values, there is no need to exclude any observations from the dataset.</p>

<h2>Check for outliers</h2>

<p>Graphical analysis of variables&#39; distributions done on Data Understanding step did not show any suspicious outliers.</p>

<h2>Creation of dummy variables</h2>

<p>Previously we identified, that two variables are categorical now: Department and Salary. Now we convert them to dummy variables, creating two dummy variables for Salary and nine dummy variables for Department.</p>

<pre><code class="r">#---dummy &lt;- dummyVars(&quot; ~ .&quot;, data = HR, fullRank = TRUE)
#---HR.dm &lt;- data.frame(predict(dummy, newdata = HR))
#---str(HR.dm)
</code></pre>

<h2>Range normalization</h2>

<p>Classification tree methodology does not require scale normalization of input data, so we do not perform this step.</p>

<h2>Dimensionality reduction</h2>

<p>Having small number of variables, we decided to avoid dimensionality reduction. </p>

<h2>Correction of variables format</h2>

<p>For using classification tree methodology, we convert part of variables to factor type.</p>

<pre><code class="r">#---columns_to_factors &lt;- c(6:19)
#---HR.dm[,columns_to_factors] &lt;- lapply(HR.dm[,columns_to_factors], factor)
#---str(HR.dm)
</code></pre>

<h2>Separation into training and test datasets</h2>

<p>Let&#39;s separate dataset into 80% training dataset and 20% test dataset.</p>

<pre><code class="r">#---set.seed(777)
#---train.index &lt;- createDataPartition(HR$left, p = .8, list = FALSE, times = 1)
#---TrainDS &lt;- HR.dm[ train.index, ]
#---TestDS  &lt;- HR.dm[-train.index, ]
</code></pre>

<p>There are observations in training dataset and observations in test dataset.</p>

<p>We can check that both datasets are roughly similar in distribution of target variable (whether employee left).</p>

<pre><code class="r">#---round(prop.table(table(TrainDS$left)),3)
#---round(prop.table(table(TestDS$left)),3)
</code></pre>

<h1>Modelling: Decision Tree</h1>

<h2>Choice of methodology and tool</h2>

<p>Possible methodologies for solving classification problems include logistic regression, support vector machine, decision tree, Naive Bayes classification, k-nearest neighbor. For the purpose of this exercise we will use decision tree methodology. It should provide an output in a form of clear set of rules, that should be easy to communicate to management and easy to implement (for example, in company&#39;s HR system).</p>

<p>Our model should predict categorical response variable (if employee leaves) with two possible outcomes: yes (1) or no (0). For that reason, decision tree should be a classification tree. </p>

<p>There are many packages in R for modeling decision trees: for example, rpart, party, RWeka, ipred, randomForest, gbm, C5.0. C5.0 is one of most up-to-date algorithms with following advantages (<a href="http://en.proft.me/2016/11/9/classification-using-decision-trees-r/">source</a>):</p>

<ul>
<li>An all-purpose classifier that does well on most problems.</li>
<li>Highly automatic learning process, which can handle numeric or nominal features, as well as missing data.</li>
<li>Less data cleaning required.</li>
<li>Excludes unimportant features.</li>
<li>Can be used on both small and large datasets.</li>
<li>Non parametric method (have no assumptions about the space distribution and the classifier structure).</li>
<li>Results in a model that can be interpreted without a mathematical background (for relatively small trees).</li>
<li>More efficient than other complex models.</li>
</ul>

<p>C4.5, which was a C5.0 predecessor, became quite popular after ranking #1 in the Top 10 Algorithms in Data Mining pre-eminent paper published by Springer LNCS in 2008 (<a href="https://en.wikipedia.org/wiki/C4.5_algorithm">source</a>). Taking all advantages into account, we are using C5.0 algorithm for the modelling.</p>

<p>C5.0 builds decision trees from a set of training data using the concept of information entropy. At each node of the tree, C5.0 chooses the attribute of the data that most effectively splits its set of samples into subsets enriched in one class or the other. The splitting criterion is the normalized information gain (difference in entropy). The attribute with the highest normalized information gain is chosen to make the decision. The C5.0 algorithm then recurs on the smaller sublists.</p>

<h2>First application of tool to dataset</h2>

<p>For a first iteration, we apply C5.0 algorithm to the whole training dataset. Variable &quot;Leave&quot; serves as target variables and all other variables - as predictors. After the initial modelling we&#39;ll see which predicting variables are less helpful in predicting leave of employees and can be removed from modelling process.</p>

<pre><code class="r">#---ClTree1 &lt;- C5.0(TrainDS[-7],TrainDS$left)
#---summary(ClTree1)
</code></pre>

<h2>Simplification of the model</h2>

<p>As we see from results of the first iteration, only five variables are mainly used by the model for making sufficiently precise prediction: average_montly_hours; satisfaction_level;   time_spend_company; last_evaluation; number_project.</p>

<p>We will build a model only with these variables and will later evaluate performance of this simplified model on test dataset.</p>

<pre><code class="r">#---ClTree2 &lt;- C5.0(TrainDS[c(4, 1, 5, 2, 3)],TrainDS$left)
#---summary(ClTree2)
</code></pre>

<pre><code class="r"># save results of modeling to .txt file
# data saved in file enables visualization of tree using free GraphViz software
# GraphViz can be installed from http://www.graphviz.org/
#---C5.0.graphviz(ClTree2, &#39;.//Tree2.txt&#39;)
</code></pre>

<h1>Evaluation</h1>

<p>To evaluate the model we apply it to test dataset and build the confusion matrix of results. It shows how many mistakes of different type the model does and how many observations are classified correctly.</p>

<pre><code class="r">#---PredictionTree &lt;- predict(ClTree2, TestDS)
#---CT.Tree &lt;- CrossTable(TestDS$left, PredictionTree, prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE, dnn = c(&#39;actual leave&#39;, &#39;predicted leave&#39;))
#---CT.Tree.DF &lt;- as.data.frame(CT.Tree)
</code></pre>

<p>As we can see from the table, for [ ] out of [ ] observations in test dataset model correctly predicted if employee will leave. This includes [ ] observations where model correctly predicted that employee will not leave and [ ] observation where model correctly predicted leave. Thus [ ] observations from test dataset were classified correctly.</p>

<p>Dataset was initially imbalanced ([ ] of all employees in dataset did not leave). Even that we already see significant information gain vs random gassing from conclusion above, let&#39;s check how successfully model identifies observations of both positive and negative target variable.</p>

<p>Model correctly identified [ ]% of positive observations (the ones where employee left). Only [ ] of employees who in fact left were not identified by the model.</p>

<p>Model also correctly identified [ ]% of negative observations. Only for [ ] of employees model generated false positive prediction.</p>

<h1>Deployment</h1>

<p>As a result of the project, we built a model that proved to be a successful predictor of possible leave of employees. It turned out, that most powerful predictors of leave are working hours, satisfaction level, time spent at company, last evaluation, and number of projects. But this data should be used in a systematic logical way, described by decision tree we built - none of variables along can not predict a leave (this we saw from correlation analysis at the beginning).</p>

<p>Should this model be deployed in a real business environment, it can be integrated into HR IT system - letting know HR and line managers about risk of each of employees&#39; leave. Being implemented in HR system, it can also generate a report, which signs were used by the algorithm to predict the leave (is it because employee work critically long hours or because of combination of big number of projects and low satisfaction?).</p>

<p>Potential ways to improve this project include the following.</p>

<ul>
<li>It&#39;s possible to build predictive models using other methodologies and tools, in order to find ways to improve predictive power even further.</li>
<li>It makes sense to integrate estimation of employee&#39;s value for the company in the model - so that company will try to stop from leaving only employees who generate enough positive results.</li>
<li>Tool can be further improved to advice HR manager what parameters of employee&#39;s work-life should be changes the first in order to decrease risk of leaving most significantly.</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

    <slide class="backdrop"></slide>
  </slides>
  <div class="pagination pagination-small" id='io2012-ptoc' style="display:none;">
    <ul>
      <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=1 title='Project Objective:'>
         1
      </a>
    </li>
  </ul>
  </div>  <!--[if IE]>
    <script 
      src="http://ajax.googleapis.com/ajax/libs/chrome-frame/1/CFInstall.min.js">  
    </script>
    <script>CFInstall.check({mode: 'overlay'});</script>
  <![endif]-->
</body>
  <!-- Load Javascripts for Widgets -->
  
  <!-- LOAD HIGHLIGHTER JS FILES -->
  <script src="http://slidifylibraries2.googlecode.com/git/inst/libraries/highlighters/highlight.js/highlight.pack.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
  <!-- DONE LOADING HIGHLIGHTER JS FILES -->
   
  </html>